#!/usr/bin/env python3
from __future__ import annotations

import argparse, json, time, importlib.util
from pathlib import Path
from typing import Dict, Any, Iterable, Tuple

from openai import OpenAI


def iter_jsonl_text(txt: str) -> Iterable[dict]:
    for ln in txt.splitlines():
        ln = ln.strip()
        if ln:
            yield json.loads(ln)

def _get(o, k: str):
    if o is None:
        return None
    if isinstance(o, dict):
        return o.get(k)
    return getattr(o, k, None)

def load_map(map_path: Path) -> Dict[str, dict]:
    out: Dict[str, dict] = {}
    with map_path.open("r", encoding="utf-8", errors="replace") as f:
        for ln in f:
            ln = ln.strip()
            if not ln:
                continue
            r = json.loads(ln)
            out[r["custom_id"]] = r
    return out


def extract_response_output_text(resp_body: dict) -> str:
    """
    Extract assistant text from a Responses API response body.
    Prefer output_text if present; otherwise walk output[] content[].
    """
    if not isinstance(resp_body, dict):
        return ""
    ot = resp_body.get("output_text")
    if isinstance(ot, str) and ot.strip():
        return ot.strip()

    out = resp_body.get("output")
    if not isinstance(out, list):
        return ""

    chunks: list[str] = []
    for item in out:
        if not isinstance(item, dict):
            continue
        content = item.get("content")
        if not isinstance(content, list):
            continue
        for c in content:
            if not isinstance(c, dict):
                continue
            # responses content blocks usually look like {"type":"output_text","text":"..."}
            t = c.get("text")
            if isinstance(t, str) and t.strip():
                chunks.append(t.strip())
    return "\n".join(chunks).strip()


def parse_args():
    ap = argparse.ArgumentParser()
    ap.add_argument("--run_dir", required=True, help="batch run dir containing batch_id.txt and map.jsonl")
    ap.add_argument("--batch_id", default="", help="override batch id (else read run_dir/batch_id.txt)")
    ap.add_argument("--poll", action="store_true", help="poll until completed/failed")
    ap.add_argument("--poll_s", type=int, default=30)
    return ap.parse_args()


def main():
    a = parse_args()
    run_dir = Path(a.run_dir)
    if not run_dir.exists():
        raise FileNotFoundError(str(run_dir))

    batch_id = a.batch_id.strip()
    if not batch_id:
        batch_id = (run_dir / "batch_id.txt").read_text(encoding="utf-8").strip()

    map_path = run_dir / "map.jsonl"
    if not map_path.exists():
        raise FileNotFoundError(str(map_path))

    # Load PassC validator helpers from existing script
    passc_script = Path("/home/ubuntu/resse-core/fm_teacher/scripts/eval_passC_prompt_v0.py")
    spec = importlib.util.spec_from_file_location("pc", str(passc_script))
    pc = importlib.util.module_from_spec(spec)  # type: ignore
    assert spec and spec.loader
    spec.loader.exec_module(pc)  # type: ignore

    m = load_map(map_path)

    client = OpenAI()

    while True:
        b = client.batches.retrieve(batch_id)
        status = getattr(b, "status", None)
        output_file_id = getattr(b, "output_file_id", None)
        error_file_id = getattr(b, "error_file_id", None)
        batch_id = getattr(b, "id", None)
        print("batch_id", batch_id, "status", status, "output_file_id", output_file_id, "error_file_id", error_file_id)

        if not a.poll:
            break
        if status in ("completed", "failed", "cancelled", "expired"):
            break
        time.sleep(a.poll_s)

    if status != "completed":
        print("NOT_COMPLETED; nothing to collect")
        return

    if not output_file_id:
        print("MISSING output_file_id")
        return

    out_txt = extract_batch_output_text(client, output_file_id)

        # Save raw batch output for debugging/audit
    raw_out_path = run_dir / "collected" / "output.jsonl"
    raw_out_path.write_text(out_txt, encoding="utf-8")
    print("wrote_raw_output", str(raw_out_path))

    out_dir = run_dir / "collected"
    out_dir.mkdir(parents=True, exist_ok=True)
    ok_path = out_dir / "ok.jsonl"
    bad_path = out_dir / "bad.jsonl"

    ok = bad = 0
# out_txt is JSONL of batch envelopes: {id, custom_id, response{status_code, body{...}}, error}
lines = [ln for ln in out_txt.splitlines() if ln.strip()]

with ok_path.open("w", encoding="utf-8") as okf, bad_path.open("w", encoding="utf-8") as badf:
    for ln in lines:
        try:
            env = json.loads(ln)
        except Exception as e:
            badf.write(json.dumps({"error": f"json_parse_error:{type(e).__name__}", "raw_line": ln[:500]}, ensure_ascii=False) + "\n")
            continue

        custom_id = env.get("custom_id") or ""
        resp = env.get("response") or {}
        status_code = resp.get("status_code")
        body = resp.get("body") or {}
        err = env.get("error")

        # Map custom_id -> meta (cluster_id, domain, facts list) using your existing map loader
        meta = id_to_meta.get(custom_id)
        if not meta:
            badf.write(json.dumps({"custom_id": custom_id, "error": "missing_custom_id_mapping"}, ensure_ascii=False) + "\n")
            continue

        domain = meta["domain"]
        cluster_id = meta["cluster_id"]
        facts = meta["facts"]
        facts_by_i = {f["i"]: f["text"] for f in facts}
        presented = list(facts_by_i.keys())

        # Hard failure from batch API
        if err or status_code != 200:
            badf.write(json.dumps({
                "domain": domain,
                "cluster_id": cluster_id,
                "facts": facts,
                "raw": "",
                "validation_errors": [f"batch_status:{status_code}", f"batch_error:{err}"],
            }, ensure_ascii=False) + "\n")
            continue

        text = extract_response_output_text(body)
        try:
            obj = json.loads(text) if text else None
        except Exception as e:
            obj = None
            parse_err = f"model_json_parse_error:{type(e).__name__}:{str(e)[:160]}"
        else:
            parse_err = ""

        if not isinstance(obj, dict):
            badf.write(json.dumps({
                "domain": domain,
                "cluster_id": cluster_id,
                "facts": facts,
                "raw": text,
                "validation_errors": [parse_err or "empty_or_nonobject_model_output"],
            }, ensure_ascii=False) + "\n")
            continue

        errs = pc.validate_obj(obj, k=int(meta.get("k", 6)), presented_idx=presented, facts_text_by_i=facts_by_i)
        rec = {
            "domain": domain,
            "cluster_id": cluster_id,
            "facts": facts,
            "raw": text,
            "obj": obj,
            "validation_errors": errs,
        }
        if errs:
            badf.write(json.dumps(rec, ensure_ascii=False) + "\n")
        else:
            okf.write(json.dumps(rec, ensure_ascii=False) + "\n")
