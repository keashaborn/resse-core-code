#!/usr/bin/env python3
from __future__ import annotations

import argparse, json, os, time, random, subprocess
from pathlib import Path
from typing import Dict, List, Any, Iterable
from openai import OpenAI

# Optional coverage ledger (only used if RFF_LEDGER_DB is set)
try:
    import scripts.rff_coverage_sqlite_v0 as led  # when run from repo root
except Exception:
    try:
        import rff_coverage_sqlite_v0 as led       # when run from scripts/ or PYTHONPATH
    except Exception:
        led = None



QDRANT = "http://127.0.0.1:6333"
COLL = "sc_fractal_field_facts_v1"
EMB_MODEL = "text-embedding-3-large"

def iter_jsonl(path: Path) -> Iterable[dict]:
    with path.open("r", encoding="utf-8") as f:
        for ln in f:
            ln = ln.strip()
            if ln:
                yield json.loads(ln)

def write_jsonl_append(path: Path, obj: dict):
    with path.open("a", encoding="utf-8") as f:
        f.write(json.dumps(obj, ensure_ascii=False) + "\n")

def qdrant_search(vec: List[float], domain: str, limit: int) -> List[dict]:
    body = {
        "vector": vec,
        "limit": limit,
        "with_payload": True,
        "with_vector": False,
        "filter": {"must": [{"key": "domain", "match": {"value": domain}}]},
    }
    out = subprocess.check_output(
        ["curl", "-sS", "-X", "POST",
         f"{QDRANT}/collections/{COLL}/points/search",
         "-H", "Content-Type: application/json",
         "-d", json.dumps(body)],
        text=True,
    )
    data = json.loads(out)
    return (data.get("result") or [])

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--field_dir", required=True, help="PassMâ€² output dir (passMprime_v0_...)")
    ap.add_argument("--out_root", default="/home/ubuntu/resse-core/fm_teacher/runs")
    ap.add_argument("--n_concepts", type=int, default=100)
    ap.add_argument("--queries_per_concept", type=int, default=3)
    ap.add_argument("--hits_per_query", type=int, default=15)
    ap.add_argument("--max_members", type=int, default=30)
    ap.add_argument("--seed", type=int, default=7)
    args = ap.parse_args()

    random.seed(args.seed)

        # Optional coverage ledger (coverage-first sampling)
    ledger_db = os.environ.get("RFF_LEDGER_DB", "").strip()
    use_ledger = bool(ledger_db) and (led is not None)
    conn = None
    seen = set()
    if use_ledger:
        conn = led.open_db(ledger_db)
        seen = led.get_seen_set(conn)
        print(f"RFF_LEDGER enabled db={ledger_db} seen={len(seen)}")
    else:
        if ledger_db and led is None:
            print("RFF_LEDGER requested but ledger module not importable; continuing without ledger")

    field_dir = Path(args.field_dir)
    nodes_path = field_dir / "concept_nodes.jsonl"
    seeds_path = field_dir / "retrieval_seeds.jsonl"
    if not nodes_path.exists() or not seeds_path.exists():
        raise SystemExit("field_dir must contain concept_nodes.jsonl and retrieval_seeds.jsonl")

    # concept_id -> canonical_text, domain
    concept = {}
    for n in iter_jsonl(nodes_path):
        concept[n["concept_id"]] = {"canonical_text": n["canonical_text"], "domain": n["domain"]}

    # read seeds
    seed_rows = [r for r in iter_jsonl(seeds_path) if r.get("concept_id") in concept and r.get("retrieval_queries")]
    if not seed_rows:
        raise SystemExit("No usable retrieval_seeds rows found.")

    # sample concepts (dedupe by concept_id)
    by_cid: Dict[str, dict] = {}
    for r in seed_rows:
        cid = r["concept_id"]
        if cid not in by_cid:
            by_cid[cid] = r

cids_all = list(by_cid.keys())
random.shuffle(cids_all)

if use_ledger:
    unseen = [cid for cid in cids_all if cid not in seen]
    random.shuffle(unseen)
    picked = unseen[: args.n_concepts]
    if len(picked) < args.n_concepts:
        # backfill with any remaining (including already-seen) to hit target n_concepts
        need = args.n_concepts - len(picked)
        backfill = [cid for cid in cids_all if cid not in set(picked)]
        picked.extend(backfill[:need])
    cids = picked
    print(f"RFF_LEDGER sampling picked={len(cids)} unseen_used={min(len(unseen), args.n_concepts)}")
else:
    cids = cids_all[: args.n_concepts]

    ts = time.strftime("%Y%m%d_%H%M%S")
    run_dir = Path(args.out_root) / f"passX_mini_v0_{ts}"
    clusters_dir = run_dir / "clusters_by_domain_passX"
    clusters_dir.mkdir(parents=True, exist_ok=True)

    client = OpenAI(timeout=90.0)

    made = 0
    for cid in cids:
        dom = concept[cid]["domain"]
        if use_ledger and conn is not None:
            led.mark_expanded(conn, cid, dom, str(field_dir))
            # commit occasionally to keep WAL small and avoid data loss if killed
            if (made % 100) == 0:
                conn.commit()

        canon_text = concept[cid]["canonical_text"]
        rq = (by_cid[cid].get("retrieval_queries") or [])[: args.queries_per_concept]

        # embed queries (resilient): fall back on 403 instead of killing the iteration
        if not rq:
            continue

        emb_model = os.environ.get("PASSX_EMB_MODEL", EMB_MODEL)
        fallback_model = os.environ.get("PASSX_EMB_FALLBACK", "text-embedding-3-small")

        def _embed(model_name: str):
            return client.embeddings.create(model=model_name, input=rq)

        try:
            emb = _embed(emb_model)
        except Exception as e:
            code = getattr(e, "status_code", None)
            msg = str(e).lower()

            # If the primary embedding model is not permitted, try fallback once.
            if code == 403 or "permission" in msg:
                try:
                    emb = _embed(fallback_model)
                    print(f"PASSX_EMB_FALLBACK_OK concept_id={cid} domain={dom} primary={emb_model} fallback={fallback_model}")
                except Exception as e2:
                    code2 = getattr(e2, "status_code", None)
                    print(f"PASSX_EMB_SKIP concept_id={cid} domain={dom} primary={emb_model} fallback={fallback_model} err2={type(e2).__name__} code2={code2}")
                    continue
            else:
                # transient-ish errors: retry a few times then skip
                transient = code in (429, 500, 502, 503, 504) or any(
                    s in msg for s in ("timeout", "timed out", "readtimeout", "overloaded", "rate limit", "connection")
                )
                if transient:
                    ok = False
                    for attempt in range(1, 7):
                        time.sleep(min(30.0, (2 ** (attempt - 1)) + random.random()))
                        try:
                            emb = _embed(emb_model)
                            ok = True
                            break
                        except Exception:
                            continue
                    if not ok:
                        print(f"PASSX_EMB_TRANSIENT_GIVEUP concept_id={cid} domain={dom} model={emb_model} rq_n={len(rq)}")
                        continue
                else:
                    raise

        vecs = [d.embedding for d in emb.data]

        # search each query, dedupe by fact_id
        seen_fact = set()
        members = []
        for v in vecs:
            hits = qdrant_search(v, dom, args.hits_per_query)
            for h in hits:
                payload = h.get("payload") or {}
                fid = payload.get("fact_id") or h.get("id")
                if not fid or fid in seen_fact:
                    continue
                seen_fact.add(fid)
                txt = payload.get("text") or ""
                if not txt:
                    continue
                members.append({
                    "text": txt,
                    "fact_id": fid,
                    "dup_group_id": payload.get("dup_group_id") or payload.get("fingerprint"),
                })
                if len(members) >= args.max_members:
                    break
            if len(members) >= args.max_members:
                break

        # write cluster record into domain file
        if members:
            out_path = clusters_dir / f"{dom}.jsonl"
            rec = {
                "seed_fact_id": f"passx::{cid}",
                "seed_payload": {"text": canon_text},
                "members": members,
                "meta": {
                    "source_field_dir": str(field_dir),
                    "concept_id": cid,
                    "domain": dom,
                    "retrieval_queries": rq,
                }
            }
            write_jsonl_append(out_path, rec)
            made += 1

    print("WROTE_RUN_DIR", str(run_dir))
    print("CLUSTERS_DIR", str(clusters_dir))
    print("clusters_written", made)
    print("\nNext step (run PassC on expansion clusters):")
    print(f'python3 /home/ubuntu/resse-core/fm_teacher/scripts/eval_passC_prompt_v0.py --clusters_dir "{clusters_dir}" --domains "$(ls -1 {clusters_dir}/*.jsonl | sed \'s#.*/##; s#\\.jsonl$##\' | paste -sd, -)" --n_per_domain 999999 --system_prompt_file /home/ubuntu/resse-core/fm_teacher/prompts/passC_system_v0.txt --schema_file /home/ubuntu/resse-core/fm_teacher/schemas/passC_schema_v0.json')

if __name__ == "__main__":
    main()
